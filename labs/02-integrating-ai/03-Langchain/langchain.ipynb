{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Langchain\n",
    "\n",
    "In this lab, we will introduce [Langchain](https://python.langchain.com/docs/get_started/introduction), a framework for developing applications powered by language models.\n",
    "\n",
    "Langchain supports Python and Javascript / Typescript. For this lab, we will use Python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by importing the `AzureOpenAI` specific components from the `langchain` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import AzureOpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with all the other labs, we'll need to provide our API key and endpoint details, so we'll load them from our `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azure_ad\n",
      "eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6Ii1LSTNROW5OUjdiUm9meG1lWm9YcWJIWkdldyIsImtpZCI6Ii1LSTNROW5OUjdiUm9meG1lWm9YcWJIWkdldyJ9.eyJhdWQiOiJodHRwczovL2NvZ25pdGl2ZXNlcnZpY2VzLmF6dXJlLmNvbSIsImlzcyI6Imh0dHBzOi8vc3RzLndpbmRvd3MubmV0LzNmNDM5YzY1LTZkMzItNDc3OC04ZTUzLTY4MzYyOWM5MzU1OC8iLCJpYXQiOjE2OTIzODYwMDksIm5iZiI6MTY5MjM4NjAwOSwiZXhwIjoxNjkyMzkwNDUzLCJhY3IiOiIxIiwiYWlvIjoiQVZRQXEvOFVBQUFBMHhmR3YyRGtUbVRBL1Z6QzdSUk9qdk9yRWdoOUxQWW0rZVQ5WjZZa3F0dUtnbjl1VGtiOGdRbjczQXRNaWltc1g3c0NDZFhhMERObndxS21xNFlNcERKaHJ3RXFIUE1BS2pKVUtrd2NoRFk9IiwiYW1yIjpbInB3ZCIsIm1mYSJdLCJhcHBpZCI6IjA0YjA3Nzk1LThkZGItNDYxYS1iYmVlLTAyZjllMWJmN2I0NiIsImFwcGlkYWNyIjoiMCIsImZhbWlseV9uYW1lIjoiV2hpdGJ5IiwiZ2l2ZW5fbmFtZSI6Ik1hcmsiLCJncm91cHMiOlsiODQ4ZmJhOGUtNjNlYS00ZmNiLWFjZTYtYWVhYWEyNTY2NTI5IiwiYmQyYWRkYzgtMTQ4MC00MjMyLTg5ZDgtNjBkY2E1NGVlMWJlIl0sImlwYWRkciI6IjJhMDA6MjNlZToxNGYwOjRmMTo0MGUyOjIyYjA6YmRkYzo4YjdhIiwibmFtZSI6Ik1hcmsgV2hpdGJ5Iiwib2lkIjoiYTFiNDhjOTMtNTliNy00ODA2LTljMjgtM2RhNzhlMGZjYjhjIiwicHVpZCI6IjEwMDMyMDAyMzE1MUUwQkYiLCJyaCI6IjAuQVgwQVpaeERQekp0ZUVlT1UyZzJLY2sxV0pBaU1YM0lLRHhIb08yT1UzU2JiVzJjQUNRLiIsInNjcCI6InVzZXJfaW1wZXJzb25hdGlvbiIsInN1YiI6Im41UExmZmpXcXB2M2stbFNnQmE2blNoRXJrU1VIVm1OdVhrNHhNYXhWSzAiLCJ0aWQiOiIzZjQzOWM2NS02ZDMyLTQ3NzgtOGU1My02ODM2MjljOTM1NTgiLCJ1bmlxdWVfbmFtZSI6Im1AbXNhenVyZS5kZXYiLCJ1cG4iOiJtQG1zYXp1cmUuZGV2IiwidXRpIjoiVlRhTlE4N3prRS0wNUhJSEtxOTJBQSIsInZlciI6IjEuMCIsIndpZHMiOlsiNjJlOTAzOTQtNjlmNS00MjM3LTkxOTAtMDEyMTc3MTQ1ZTEwIiwiYjc5ZmJmNGQtM2VmOS00Njg5LTgxNDMtNzZiMTk0ZTg1NTA5Il19.XxfUFsqULzz-mtvkViwc8IIXfYYJuurSLC8uLSAxBaL6v1JnjfR9DWQO1HQNTOQTJPSrL0kJFyzjxTMF7x6FUE0gswdyVV_OS1JjMP9DjFNsENn0BWaTMROm7du4hJ0jnK_xQLdUNsebIMD9OuvJqhlJ2OsW0euL1zQUWi0YqDD45459EETX91PpJi1cHUoBrGpAiF_TbjERWOyQmf7yFFht4ILpdRvNV0hm-e87je4sa6y3-7CE5o55fE0rg9DstJ8_BWJWAhWcAqQpLc8Eh22nSZbKmviJOYhxRuXHVgK_r52FUC1epSSRq5uBcxyz2EJAnJrIOFlGvtTQEfTNCQ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "\n",
    "if openai.api_type == \"azure_ad\":\n",
    "    default_credential = DefaultAzureCredential()\n",
    "    token = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "    openai.api_key = token.token\n",
    "else:\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install azure-identity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll configure Langchain by providing the API key and endpoint details, along with the API version information.\n",
    "\n",
    "As Langchain can work with multiple AI services, we need to specify that we want to work with Azure via the `openai_api_type` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of Azure OpenAI\n",
    "llm = AzureOpenAI(\n",
    "    openai_api_type = openai.api_type,\n",
    "    openai_api_version = os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    openai_api_base = os.getenv(\"OPENAI_API_BASE\"),\n",
    "    openai_api_key = openai.api_key,\n",
    "    deployment_name = os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a prompt to Azure OpenAI using Langchain\n",
    "\n",
    "We're now ready to send a request to Azure OpenAI. To do this, we invoke the `llm` instance we created above and pass in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90% timing and 10% delivery.\"  You can have the right jokes, but if the timing is off, it's not going to land.  And if the delivery is off, it's not going to land.  It's a balancing act, and just like anything else, it takes practice.  \n",
      "\n",
      "So, what can you do to improve your comedic timing and delivery? \n",
      "\n",
      "1. Watch comedy: One of the best ways to learn comedic timing and delivery is to watch comedians and see how they do it.  Pay attention to when they pause, when they speed up, and when they slow down.  Also, pay attention to how they deliver their jokes.  Do they use different voices or facial expressions? Do they use props? \n",
      "\n",
      "2. Practice: It's one thing to watch comedy, but it's another thing to actually practice it.  Write down some jokes and practice delivering them in front of a mirror or to a friend.  Pay attention to your timing and delivery.  If it's not landing, try changing things up until you find what works. \n",
      "\n",
      "3. Pay attention to your audience: One of the keys to good comedic timing is knowing your audience.  Pay attention to their reactions and adjust accordingly.  If they\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call the API\n",
    "r = llm(\"The secret of great comedy is \")\n",
    "\n",
    "# Print the response\n",
    "print(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a prompt to Azure OpenAI using Langchain Chaining\n",
    "\n",
    "Now that we have seen Langchain in action, let's take a quick peek at chaining and adding variables to our prompt. To do this we will add `LLMChain` to the `llm` instance created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the OpenAI API, we still had to pass the prompt in using the `Completion.create()` method. With Langchain, we can create a `PromptTemplate`. This way, we can define our prompt up front and leave placeholders for values that will be set later on. The placeholder could be values that are passed from an end user or application via an API. We don't know what they at this point.\n",
    "\n",
    "In the below example, the `{input}` in curly brackets is the placeholder value that will be populated later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template with variables, note the curly braces\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"What interesting things can I make with a {input}?\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a chain. In this case, the chain has two components. One component is the prompt template. The other is the object that represents our AI model (`llm`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we initiate the chain. You can see that we pass in a value for the `input` placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chain only specifying the input variable.\n",
    "response = chain.run({\"input\": \"raspberry pi\"})\n",
    "\n",
    "# As we are using a single input variable, you could also run the string like this:\n",
    "# response = chain.run(\"raspberry pi\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Langchain is an example of an AI orchestrator. It provides an alternative method to the raw API or using an SDK package to access the AI models, but on top of that can provide additional integrations, deal with issues related to rate limiting of the API and provide an abstraction layer over potentially complex operations. We'll get into those more complex use cases in later labs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "In the next lab, we'll look at another AI Orchestrator - Semantic Kernel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
